---
layout: archive
title: "Prospective Students"
permalink: /prospective/
author_profile: true
---

## Prospective Students

I am currently receiving a high volume of emails regarding prospective students. I have written this page to address any questions you may have. If you have additional questions that I have not answered below, feel free to email me with the header [Prospective Student].

### What do I work on?
My area of research is Natural Language Processing (NLP). My main focus is on Responsible NLP, which encompasses several subfields in NLP. My work is very interdisciplinary and as a result, I have collaborated with researchers in public health, sociology, social work, and HCI. I have listed my research interests below:

1. **Fairness**: This focuses on detecting and mitigating social biases in natural language systems. I am especially interested in cross-cultural fairness issues, which may include biases in non-English and/or non-American settings. Some related papers include:
    - **Sharon Levy**, Neha Anna John, Ling Liu, Yogarshi Vyas, Jie Ma, Yoshinari Fujinuma, Miguel Ballesteros, Vittorio Castelli, Dan Roth. "Comparing Biases and the Impact of Multilingual Training across Multiple Languages". In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP 2023), Long Paper, ACL.[[paper]](https://arxiv.org/abs/2305.11242)
    - Sophie Groenwold\*, Lily Ou\*, Aesha Parekh\*, Samhita Honnavalli\*, **Sharon Levy**, Diba Mirza and William Yang Wang. "Investigating African-American Vernacular English in Transformer-Based Text Generation" In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP 2020), Short Paper, ACL. [[paper]](https://aclanthology.org/2020.emnlp-main.473/)
    - Samhita Honnavalli\*, Aesha Parekh\*, Lily Ou\*, Sophie Groenwold\*, **Sharon Levy**, Vicente Ordonez and William Yang Wang. “Towards Understanding Gender-Seniority Compound Bias in Natural Language Generation”. Proceedings of The 13th Language Resources and Evaluation Conference (LREC 2022). [[paper]](https://aclanthology.org/2022.lrec-1.177/)
2. **Safety**: We have already seen cases in which NLP models have generated harmful information to users. This line of research studies issues related to user safety such as covertly unsafe text, i.e. text that is implicitly harmful and requires commonsense reasoning to deem it unsafe. Relevant papers include:
    - **Sharon Levy**, Emily Allaway, Melanie Subbiah, Lydia Chilton, Desmond Patton, Kathleen McKeown and William Yang Wang. "SafeText:
    - Alex Mei\*, Anisha Kabir\*, **Sharon Levy**, Melanie Subbiah, Emily Allaway, John N. Judge, Desmond Patton, Bruce Bimber, Kathleen McKeown and William Yang Wang. "Mitigating Covertly Unsafe Text within Natural Language Systems". In Findings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP 2022).[[paper]](https://aclanthology.org/2022.findings-emnlp.211/)
    - Alex Mei*, **Sharon Levy**\*, William Yang Wang. "Foveate, Attribute, and Rationalize: Towards Safe and Trustworthy AI". Findings of the Association for Computational Linguistics (ACL 2023)[[paper]](https://arxiv.org/abs/2212.09667)
    - Alex Mei*, **Sharon Levy**\*, William Yang Wang. "ASSERT: Automated Safety Scenario Red Teaming for Evaluating the Robustness of Large Language Models". In Findings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP 2023)[[paper]](https://arxiv.org/abs/2310.09624)
3. **Trustworthiness**: As natural language systems are trained, they may learn information that becomes outdated over time or is incorrect in general. We focus on both harmful (e.g. conspiracy theories) and less harmful (e.g. satire) types of misinformation. The following lists related papers:
    - **Sharon Levy**, Michael Saxon, William Yang Wang. "Investigating Memorization of Conspiracy Theories in Text Generation". Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. [[paper]](https://aclanthology.org/2021.findings-acl.416/) [[blog post]](http://nlp.cs.ucsb.edu/blog/investigating-memorization-of-conspiracy-theories-in-text-generation.html)
    - **Sharon Levy**\*, Kai Nakamura\*, William Yang Wang. "r/Fakeddit: A New Multimodal Benchmark Dataset for Fine-grained Fake News Detection". Proceedings of The 12th Language Resources and Evaluation Conference (LREC), 2020. [[paper]](https://www.aclweb.org/anthology/2020.lrec-1.755/)
4. **Question Answering**: Related to trustworthiness, this focuses specifically on how natural language systems can be used as question answering applications and ensure that models can answer queries truthfully. Related papers include:
    - Matthew Ho\*, Aditya Sharma\*, Justin Chang\*, Michael Saxon, **Sharon Levy**, Yujie Lu and William Yang Wang. “WikiWhy: Answering and Explaining Cause-and-Effect Questions”. In Proceedings of the International Conference on Learning Representations (ICLR 2023), Oral Paper: Top 5% out of all 4019 submissions. [[paper]](https://arxiv.org/abs/2210.12152)
    - **Sharon Levy**, Kevin Mo, Wenhan Xiong, William Yang Wang. "Open-Domain Question-Answering for COVID-19 and Other Emergent Domains". In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations (EMNLP 2021). [[paper]](https://aclanthology.org/2021.emnlp-demo.30/)
    - Kai Nakamura, **Sharon Levy**, Yi-Lin Tuan, Wenhu Chen, William Yang Wang. “HybriDialogue: An Information-Seeking Dialogue Dataset Grounded on Tabular and Textual Data”. Findings of the Association for Computational Linguistics (ACL 2022). [[paper]](https://aclanthology.org/2022.findings-acl.41/)
    - Alon Albalak, **Sharon Levy**, William Yang Wang. "Addressing Issues of Cross-Linguality in Open-Retrieval Question Answering Systems For Emergent Domains". In Proceedings of the 2023 Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations (EACL 2023)[[paper]](https://arxiv.org/abs/2201.11153)
5. **Computational Social Science**: Here, we use NLP as a tool to study interactions in the real world and online. The following lists my relevant work:
    - **Sharon Levy**, Robert E. Kraut, Jane A. Yu, Kristen M. Altenburger, Yi-Chia Wang. "Understanding Conflicts in Online Conversations". In Proceedings of the ACM Web Conference 2022 (WWW ’22). [[paper]](https://dl.acm.org/doi/10.1145/3485447.3512131)


### What is my advising style?
I plan to work closely with my students and hold weekly 1-on-1s with each student. In addition to these meetings, I will also hold a weekly lab group in which students will provide updates on their progress in order to help form collaborations and overcome project difficulties. These meetings will also contain a reading group, where students will present interesting and relevant research papers that have been recently published to practice their presentation skills and stay up-to-date on current research. 


### Prospective PhD Students
If you are interested in working with me, please apply to the Rutgers CS department as a PhD student and specify my name as a potential advisor. 
You can find more information [[here]](https://www.cs.rutgers.edu/academics/graduate/prospective-students)
The deadline is January 1st. As I am receiving a high volume of emails, please do not email me with your CV. I will go through the applications once the deadline is over and if I see a good fit, I will contact you for an interview. 

### Current Rutgers Students
I will also be accepting students who currently study at Rutgers. If you are interested in working with me, please email me with the header [Current Rutgers Student] closer to the start of the Fall 2024 semester.

